# Awesome-instruction-tuning
A curated list of awesome instruction tuning datasets, models and papers.
## Datasets

This project will collect all existing open-source instruction tuning datasets to help the community to better fine-tune the language model.Our contribution can be listed as follows:
1. These existing datasets mainly focus on english. Based on open-source translation models, we build a pipline to translate them to other n languages. And we will provide the translated dataset successively. 
2. We release the translation code to benefit the low-resoure languages community. We don't use commercial APIs, so it's free.
3. We rewrote some Chinese datasets to fit the instruction fine-tuning format. 

Type have two options: task and chat. "task" represents that the datasets modified from natural language precessing. "chat" represents that the datasets are collected from  the Internet or generated by Chatgpt. 
| Dataset   | Type  | task size | instance Size | Language
|  ----  | ----  | ----  | ----  | ----  |
| [nature instruction](https://github.com/allenai/natural-instructions)  | task |1,616 |5M |all tasks instructions are English and dataset contain non-english|
|   |  |  |   |  |



## Models




## Papers

[**Finetuned language models are zero-shot learners**](https://arxiv.org/abs/2109.01652) 2021.9

[**Multitask Prompted Training Enables Zero-Shot Task Generalization**](https://arxiv.org/abs/2110.08207) 2021.10

[**Training language models to follow instructions with human feedback**](https://arxiv.org/abs/2203.02155) 2022.3
     
[**Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks**](https://arxiv.org/abs/2204.07705) 2022.4

[**Unsupervised Cross-Task Generalization via Retrieval Augmentation**](https://arxiv.org/abs/2204.07937) 2022.4
  
[**Instruction Induction: From Few Examples to Natural Language Task Descriptions**](https://arxiv.org/abs/2205.10782) 2022.5
    
[**Scaling Instruction-Finetuned Language Models**](https://arxiv.org/abs/2210.11416) 2022.10

[**Guess the Instruction! Flipped Learning Makes Language Models Stronger Zero-Shot Learners**](https://arxiv.org/abs/2210.02969) 2022.10
   
[**Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor**](https://arxiv.org/abs/2212.09689)  2022.12

[**Improving Cross-task Generalization of Unified Table-to-text Models with Compositional Task Configurations**](https://arxiv.org/abs/2212.08780) 2022.12

[**Self-Instruct: Aligning Language Model with Self Generated Instructions**](https://arxiv.org/abs/2212.10560) 2022.12

[**MultiInstruct: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning**](https://arxiv.org/abs/2212.10773) 2022.12

[**The Flan Collection: Designing Data and Methods for Effective Instruction Tuning**](https://arxiv.org/abs/2301.13688) 2023.1

[**In-Context Instruction Learning**](https://arxiv.org/abs/2302.14691) 2023.2
